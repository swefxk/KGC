全流程复现规范（KGC 全实体 Link Prediction）

0. 任务与最终评测口径（必须统一）

任务：全实体 Link Prediction（filtered ranking）。
- Query：RHS (h,r,?) 与 LHS (?,r,t) 两侧都评测。
- Rank：在全实体集合上比较分数，排除 filtered 真值后计算 rank → MRR/Hits。
- 汇报：RHS/LHS/AVG 的 MRR、Hits@{1,3,10}，以及 Rec@K（gold 是否进入候选 topK）。

0.1 系统推理形态（TopK injection exact-rank）

- RotatE 先在全实体上打结构分数，取 topK 候选（K=200 为主线）。
- Sem 只在 topK 上打分；Gate 给出注入强度 g；Refiner (Δ) 只在 topK 内纠错。
- full-entity rank 用阈值 s_thresh 比较，确保定义“仍是全实体 rank”。
  s_thresh = struct_w * s_gold_struct + (b * g) * s_gold_sem
  Δ 不参与阈值；Δ 仅在 topK 内改变 candidate 的 total score。
  若开启 --gold_struct_threshold_no_sem：s_thresh 退化为 struct_w * s_gold_struct。
- 评测脚本：eval/eval_topk_inject.py（仍是 full-entity filtered rank，只是 Sem/Δ 在 topK 内计算）。
- 数值稳定：--score_eps 加到阈值上，避免批大小差导致比较翻转。

0.2 关键参数命名（避免混淆）

- --topk：系统候选规模（RotatE topK）。
- --K：Refiner 的邻居数（StructRefiner 初始化参数）。
- b 与 g：语义注入缩放与门控强度；struct_weight 为结构分数系数。
- --strict_r0：TopK 注入评测走严格 R0 逻辑（仅结构分数），用于等价验收。

0.3 严格等价验收（必须过）

- 使用同一 RotatE ckpt：
  - eval/eval_full_entity_filtered.py 保存 ranks
  - eval/eval_topk_inject.py --strict_r0 保存 ranks
- 要求 RHS/LHS ranks 完全一致，AVG MRR 完全一致。
- 一键脚本：bash scripts/check_eval_equivalence.sh <run_id> [dataset] [topk]

0.4 命名规范（写论文时必须一致）

- RotatE@TopK (strict)：eval_topk_inject --strict_r0 结果，应与 full-entity R0 一致。
- Sem (TopK rerank)：主线 Sem（用 RotatE cache 训练）在系统口径下评测。
- Sem-only (full-entity)：独立 Sem（不依赖 RotatE cache）在全实体评测。
- Sem-only@TopK：独立 Sem 在 RotatE topK 候选内重排（struct_weight=0, gamma=0, 无 gate）。

Step 1. Train RotatE（结构基座）

目的：训练结构模型 RotatE，作为 topK 召回器与 R0 基线。
输入：data_path/train.txt, valid.txt, test.txt；entities.dict, relations.dict。

命令（示例）：
python train/train_rotate.py \
  --data_path data/fb15k_custom \
  --save_dir artifacts/<run_id>/checkpoints/rotate \
  --emb_dim 1000 --margin 9.0 \
  --batch_size 1024 --num_neg 256 \
  --lr 1e-4 --epochs 200 \
  --negative_adversarial_sampling --adversarial_temperature 1.0 \
  --eval_every 10 --eval_split valid

产物：artifacts/<run_id>/checkpoints/rotate/best_model.pth

验收（R0 full-entity）：
python eval/eval_full_entity_filtered.py \
  --data_path data/fb15k_custom \
  --pretrained_rotate artifacts/<run_id>/checkpoints/rotate/best_model.pth \
  --eval_split valid --recall_k 200 \
  --out_dir artifacts/<run_id>/eval/R0_rotate_valid \
  --save_ranks_path artifacts/<run_id>/eval/R0_rotate_valid/ranks.pt \
  --test_batch_size 8 --score_eps 0

Step 2. Build Hard Negative Cache（RotatE topK negatives）

目的：为 Sem/Gate/Δ 提供 hard negatives（仅训练使用，不是模型本身）。

RHS cache：
python build/build_rotate_cache_rhs_topk.py \
  --data_path data/fb15k_custom \
  --pretrained_rotate artifacts/<run_id>/checkpoints/rotate/best_model.pth \
  --K 500 --use_filtered --filtered_splits train_valid \
  --out_path artifacts/<run_id>/cache/train_rhs_top500_neg_filtered_trainvalid.pt

LHS cache：
python build/build_rotate_cache_lhs_topk.py \
  --data_path data/fb15k_custom \
  --pretrained_rotate artifacts/<run_id>/checkpoints/rotate/best_model.pth \
  --K 500 --use_filtered_trainvalid \
  --out_path artifacts/<run_id>/cache/train_lhs_top500_neg_filtered_trainvalid.pt

产物：cache/*.pt + 对应 .json 元数据（包含 K、过滤策略与对齐说明）。

Step 3. Train Semantic Bi-Encoder（主线 Sem）

目的：在 RotatE topK hard negatives 上训练语义双塔，用于 topK 内重排。
输入：RotatE ckpt + RHS/LHS cache + entity2text/relation2text。

命令（示例）：
python train/train_sem_biencoder.py \
  --data_path data/fb15k_custom \
  --pretrained_rotate artifacts/<run_id>/checkpoints/rotate/best_model.pth \
  --train_cache_rhs artifacts/<run_id>/cache/train_rhs_top500_neg_filtered_trainvalid.pt \
  --train_cache_lhs artifacts/<run_id>/cache/train_lhs_top500_neg_filtered_trainvalid.pt \
  --save_dir artifacts/<run_id>/checkpoints/sem_biencoder \
  --neg_source cache --hard_k 128 --use_inbatch --queue_size 4096 \
  --epochs 10 --eval_every 1 \
  --eval_mode topk --eval_metric avg --eval_topk 200 \
  --eval_b_rhs 2.0 --eval_b_lhs 2.5 \
  --eval_struct_weight_rhs 1.0 --eval_struct_weight_lhs 1.0 \
  --eval_sem_only_every 5

训练/选优口径（必须同时保留）：
(A) Sem-only full-entity（消融证据）
- 由 --eval_sem_only_every 触发 eval_sem_biencoder_full，口径是全实体 filtered。
(B) System-aligned topK proxy（用于挑选 best ckpt）
- --eval_mode topk，候选由 RotatE topK 生成，评测仍是 full-entity rank。

命名规范：这条主线只能叫 “Sem (TopK rerank)”，不要写成 Sem-only。

Step 3b. Sem-only（独立训练，消融上限）

目的：得到“纯语义单体”基线（不依赖 RotatE cache）。

训练命令（示例）：
python train/train_sem_biencoder.py \
  --data_path data/fb15k_custom \
  --save_dir artifacts/<run_id>/checkpoints/sem_biencoder_indep \
  --neg_source random --random_lhs --use_inbatch \
  --epochs 10 --eval_every 1 \
  --eval_mode full --eval_metric avg \
  --eval_sem_only_every 1

评测（full-entity）：
python eval/eval_sem_biencoder_full.py \
  --data_path data/fb15k_custom \
  --pretrained_sem artifacts/<run_id>/checkpoints/sem_biencoder_indep/biencoder_best.pth \
  --eval_split test --eval_sides both \
  --topk 200 \
  --out_dir artifacts/<run_id>/eval/Sem_only_full

评测（system 口径，可选）：
python eval/eval_topk_inject.py \
  --data_path data/fb15k_custom \
  --pretrained_rotate artifacts/<run_id>/checkpoints/rotate/best_model.pth \
  --pretrained_sem artifacts/<run_id>/checkpoints/sem_biencoder_indep/biencoder_best.pth \
  --topk 200 --eval_split test --eval_sides both \
  --struct_weight_rhs 0.0 --struct_weight_lhs 0.0 \
  --refiner_gamma_rhs 0.0 --refiner_gamma_lhs 0.0 \
  --b_rhs 0.5 --b_lhs 0.5 \
  --out_dir artifacts/<run_id>/eval/Sem_only_topk

Sem-only 强化版（可选，用于审稿解释）：
- epochs 拉长、batch/inbatch 加大，仍保持 neg_source=random。
- 只要 full-entity MRR 上升一点，就足够堵“你故意训弱”的质疑。

Step 4. Train Struct Refiner（Δ）

目的：只在 topK 内输出 Δ 进行结构纠错（主线使用 candidate-aware rerank）。
输入：RotatE ckpt + cache（可选）+ 邻居拓扑（由 KGProcessor 读取）。

命令（示例）：
python train/train_struct_refiner.py \
  --data_path data/fb15k_custom \
  --pretrained_rotate artifacts/<run_id>/checkpoints/rotate/best_model.pth \
  --train_cache_rhs artifacts/<run_id>/cache/train_rhs_top500_neg_filtered_trainvalid.pt \
  --train_cache_lhs artifacts/<run_id>/cache/train_lhs_top500_neg_filtered_trainvalid.pt \
  --save_dir artifacts/<run_id>/checkpoints/refiner \
  --K 16 --hard_k 128 --sample_prefix_k 64 \
  --epochs 30 --eval_every 5 --rhs_only

说明：
- 评测时 eval_topk_inject 默认使用 score_delta_topk（R2 语义）。
- LHS 通常置 gamma_lhs=0（只对 RHS 开 Δ）。

Step 5. Train Gate（自适应注入）

目的：学习语义注入强度 g，避免固定 b 造成负优化。
输入：RotatE ckpt + Sem ckpt + cache（可选 Refiner 仅作特征）。

命令（示例）：
python train/train_gate_inject_topk.py \
  --data_path data/fb15k_custom \
  --pretrained_rotate artifacts/<run_id>/checkpoints/rotate/best_model.pth \
  --pretrained_sem artifacts/<run_id>/checkpoints/sem_biencoder/biencoder_best.pth \
  --train_cache_rhs artifacts/<run_id>/cache/train_rhs_top500_neg_filtered_trainvalid.pt \
  --train_cache_lhs artifacts/<run_id>/cache/train_lhs_top500_neg_filtered_trainvalid.pt \
  --save_dir artifacts/<run_id>/checkpoints/gate \
  --b_rhs 2.0 --b_lhs 2.5 \
  --epochs 5

产物：.../gate/gate_best.pth

Step 6. Final Eval（主表 + 消融 + bootstrap）

统一评测脚本：eval/eval_topk_inject.py
通用建议（论文数字统一开启）：
- --deterministic --disable_tf32 --matmul_precision highest --seed 42
- --batch_size 8 --score_eps 0
- 重要 run 一律 --save_ranks_path 便于比较与 bootstrap

必跑矩阵（system 口径）：
- R0 full-entity（RotatE）：eval_full_entity_filtered.py
- RotatE@TopK (strict)：eval_topk_inject --strict_r0（等价验证，不入主表）
- C：Sem+Gate（无 Δ）
- D：Sem+Gate+Δ
- Safety（可选）：--delta_gate_ent_q 0.6
- Sem-only strong@TopK（可选但推荐，用于堵审稿人）

可选消融（若需要补表）：
- Sem (TopK rerank; struct_weight=0)
- Δ-only（b=0, no gate）
- Hybrid LHS（--lhs_union_sem_topk）

Sem-only full-entity（单独表/附录）：
- 只用 eval_sem_biencoder_full.py，与 system 主表分开列出。

Paired bootstrap（显著性）：
- D − C（必须）
- C − RotatE@TopK、D − RotatE@TopK（建议）
- C − Sem-only strong@TopK（可选但很值）
命令关键参数：
--paired_bootstrap --paired_baseline_ranks <baseline>/ranks.pt

结果汇总：
- 主表与 system 对比块：artifacts/<run_id>/eval/paper_table.md
- Sem-only full-entity 独立放一张表或附录。

你们最容易被“瞎改”搞崩的三条红线（写给 Cursor）
1) Sem 训练时不能删掉 Sem-only full-entity 评测（A）。这是消融核心证据。
2) system rank 的阈值必须是 sem-preserving：s_thresh = struct_w*s_gold_struct + (b*g)*s_gold_sem（Δ 不进阈值）。
3) 任意改动 eval 逻辑后，先跑 strict 等价验收（check_eval_equivalence.sh）再汇报数字。
