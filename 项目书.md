全流程复现规范（KGC 全实体 Link Prediction）

0. 任务与最终评测口径（必须统一）

任务：全实体 Link Prediction（filtered ranking）。
- Query：RHS (h,r,?) 与 LHS (?,r,t) 两侧都评测。
- Rank：在全实体集合上比较分数，排除 filtered 真值后计算 rank → MRR/Hits。
- 汇报：RHS/LHS/AVG 的 MRR、Hits@{1,3,10}，以及 Rec@K（gold 是否进入候选 topK）。

0.1 系统推理形态（TopK injection exact-rank）

- 结构基座（RotatE/ComplEx）先在全实体上打结构分数，取 topK 候选（K=200 为主线）。
- Sem 只在 topK 上打分；Gate 给出注入强度 g；Refiner (Δ) 只在 topK 内纠错。
- full-entity rank 用阈值 s_thresh 比较，确保定义“仍是全实体 rank”。
  s_thresh = struct_w * s_gold_struct + (b * g) * s_gold_sem
  Δ 不参与阈值；Δ 仅在 topK 内改变 candidate 的 total score。
  若开启 --gold_struct_threshold_no_sem：s_thresh 退化为 struct_w * s_gold_struct。
- 评测脚本：eval/eval_topk_inject.py（仍是 full-entity filtered rank，只是 Sem/Δ 在 topK 内计算）。
- 数值稳定：--score_eps 加到阈值上，避免批大小差导致比较翻转。

0.2 关键参数命名（避免混淆）

- --topk：系统候选规模（struct topK）。
- --K：Refiner 的邻居数（StructRefiner 初始化参数）。
- b 与 g：语义注入缩放与门控强度；struct_weight 为结构分数系数。
- --strict_r0：TopK 注入评测走严格 R0 逻辑（仅结构分数），用于等价验收。
- --struct_type：结构基座类型（rotate/complex）。
- --pretrained_struct：结构基座 ckpt（优先于 --pretrained_rotate）。

0.3 严格等价验收（必须过）

- 使用同一结构基座 ckpt：
  - eval/eval_full_entity_filtered.py 保存 ranks
  - eval/eval_topk_inject.py --strict_r0 保存 ranks
- 要求 RHS/LHS ranks 完全一致，AVG MRR 完全一致。
- 一键脚本：bash scripts/check_eval_equivalence.sh <run_id> [dataset] [topk] [struct_type] [emb_dim]

0.4 命名规范（写论文时必须一致）

- RotatE@TopK (strict)：eval_topk_inject --strict_r0 结果，应与 full-entity R0 一致。
- Sem (TopK rerank)：主线 Sem（用 RotatE cache 训练）在系统口径下评测。
- Sem-only (full-entity)：独立 Sem（不依赖 RotatE cache）在全实体评测。
- Sem-only@TopK：独立 Sem 在 RotatE topK 候选内重排（struct_weight=0, gamma=0, 无 gate）。

Step 1. Train RotatE（结构基座）

目的：训练结构模型 RotatE，作为 topK 召回器与 R0 基线。
输入：data_path/train.txt, valid.txt, test.txt；entities.dict, relations.dict。

命令（示例）：
python train/train_rotate.py \
  --data_path data/fb15k_custom \
  --save_dir artifacts/<run_id>/checkpoints/rotate \
  --emb_dim 1000 --margin 9.0 \
  --batch_size 1024 --num_neg 256 \
  --lr 1e-4 --epochs 200 \
  --negative_adversarial_sampling --adversarial_temperature 1.0 \
  --eval_every 10 --eval_split valid

产物：artifacts/<run_id>/checkpoints/rotate/best_model.pth

验收（R0 full-entity）：
python eval/eval_full_entity_filtered.py \
  --data_path data/fb15k_custom \
  --pretrained_rotate artifacts/<run_id>/checkpoints/rotate/best_model.pth \
  --eval_split valid --recall_k 200 \
  --out_dir artifacts/<run_id>/eval/R0_rotate_valid \
  --save_ranks_path artifacts/<run_id>/eval/R0_rotate_valid/ranks.pt \
  --test_batch_size 8 --score_eps 0

Step 2. Build Hard Negative Cache（RotatE topK negatives）

目的：为 Sem/Gate/Δ 提供 hard negatives（仅训练使用，不是模型本身）。

RHS cache：
python build/build_rotate_cache_rhs_topk.py \
  --data_path data/fb15k_custom \
  --pretrained_rotate artifacts/<run_id>/checkpoints/rotate/best_model.pth \
  --K 500 --use_filtered --filtered_splits train_valid \
  --out_path artifacts/<run_id>/cache/train_rhs_top500_neg_filtered_trainvalid.pt

LHS cache：
python build/build_rotate_cache_lhs_topk.py \
  --data_path data/fb15k_custom \
  --pretrained_rotate artifacts/<run_id>/checkpoints/rotate/best_model.pth \
  --K 500 --use_filtered_trainvalid \
  --out_path artifacts/<run_id>/cache/train_lhs_top500_neg_filtered_trainvalid.pt

产物：cache/*.pt + 对应 .json 元数据（包含 K、过滤策略与对齐说明）。

Step 3. Train Semantic Bi-Encoder（主线 Sem）

目的：在 RotatE topK hard negatives 上训练语义双塔，用于 topK 内重排。
输入：RotatE ckpt + RHS/LHS cache + entity2text/relation2text。

命令（示例）：
python train/train_sem_biencoder.py \
  --data_path data/fb15k_custom \
  --pretrained_rotate artifacts/<run_id>/checkpoints/rotate/best_model.pth \
  --train_cache_rhs artifacts/<run_id>/cache/train_rhs_top500_neg_filtered_trainvalid.pt \
  --train_cache_lhs artifacts/<run_id>/cache/train_lhs_top500_neg_filtered_trainvalid.pt \
  --save_dir artifacts/<run_id>/checkpoints/sem_biencoder \
  --neg_source cache --hard_k 128 --use_inbatch --queue_size 4096 \
  --epochs 10 --eval_every 1 \
  --eval_mode topk --eval_metric avg --eval_topk 200 \
  --eval_b_rhs 2.0 --eval_b_lhs 2.5 \
  --eval_struct_weight_rhs 1.0 --eval_struct_weight_lhs 1.0 \
  --eval_sem_only_every 5

训练/选优口径（必须同时保留）：
(A) Sem-only full-entity（消融证据）
- 由 --eval_sem_only_every 触发 eval_sem_biencoder_full，口径是全实体 filtered。
(B) System-aligned topK proxy（用于挑选 best ckpt）
- --eval_mode topk，候选由 RotatE topK 生成，评测仍是 full-entity rank。

命名规范：这条主线只能叫 “Sem (TopK rerank)”，不要写成 Sem-only。

Step 3c. Train SCN（Semantic Confidence Net）

目的：学习 query 级语义可靠度 r(q)，只调节语义注入强度，不改变语义排序本身。
关键要求：训练时必须使用 union 候选（强制 gold 进入 Top‑K），才能稳定得到 gold_index 与“语义是否有益”的监督。

训练命令（示例）：
python train/train_sem_calibrator.py \
  --data_path data/fb15k_custom \
  --pretrained_rotate artifacts/<run_id>/checkpoints/rotate/best_model.pth \
  --pretrained_sem artifacts/<run_id>/checkpoints/sem_biencoder/biencoder_best.pth \
  --train_cache_rhs artifacts/<run_id>/cache/train_rhs_top500_neg_filtered_trainvalid.pt \
  --train_cache_lhs artifacts/<run_id>/cache/train_lhs_top500_neg_filtered_trainvalid.pt \
  --save_dir artifacts/<run_id>/checkpoints/scn \
  --topk 200 --b_rhs 2.0 --b_lhs 2.5 \
  --struct_w_rhs 1.0 --struct_w_lhs 1.0 \
  --delta_rank_margin 1 --agree_topm 10

Step 3b. Sem-only（独立训练，消融上限）

目的：得到“纯语义单体”基线（不依赖 RotatE cache）。

训练命令（示例）：
python train/train_sem_biencoder.py \
  --data_path data/fb15k_custom \
  --save_dir artifacts/<run_id>/checkpoints/sem_biencoder_indep \
  --neg_source random --random_lhs --use_inbatch \
  --epochs 10 --eval_every 1 \
  --eval_mode full --eval_metric avg \
  --eval_sem_only_every 1

评测（full-entity）：
python eval/eval_sem_biencoder_full.py \
  --data_path data/fb15k_custom \
  --pretrained_sem artifacts/<run_id>/checkpoints/sem_biencoder_indep/biencoder_best.pth \
  --eval_split test --eval_sides both \
  --topk 200 \
  --out_dir artifacts/<run_id>/eval/Sem_only_full

评测（system 口径，可选）：
python eval/eval_topk_inject.py \
  --data_path data/fb15k_custom \
  --pretrained_rotate artifacts/<run_id>/checkpoints/rotate/best_model.pth \
  --pretrained_sem artifacts/<run_id>/checkpoints/sem_biencoder_indep/biencoder_best.pth \
  --topk 200 --eval_split test --eval_sides both \
  --struct_weight_rhs 0.0 --struct_weight_lhs 0.0 \
  --refiner_gamma_rhs 0.0 --refiner_gamma_lhs 0.0 \
  --b_rhs 0.5 --b_lhs 0.5 \
  --out_dir artifacts/<run_id>/eval/Sem_only_topk

Sem-only 强化版（可选，用于审稿解释）：
- epochs 拉长、batch/inbatch 加大，仍保持 neg_source=random。
- 只要 full-entity MRR 上升一点，就足够堵“你故意训弱”的质疑。

Step 4. Train Struct Refiner（Δ）

目的：只在 topK 内输出 Δ 进行结构纠错（主线使用 candidate-aware rerank）。
输入：RotatE ckpt + cache（可选）+ 邻居拓扑（由 KGProcessor 读取）。

命令（示例）：
python train/train_struct_refiner.py \
  --data_path data/fb15k_custom \
  --pretrained_rotate artifacts/<run_id>/checkpoints/rotate/best_model.pth \
  --train_cache_rhs artifacts/<run_id>/cache/train_rhs_top500_neg_filtered_trainvalid.pt \
  --train_cache_lhs artifacts/<run_id>/cache/train_lhs_top500_neg_filtered_trainvalid.pt \
  --save_dir artifacts/<run_id>/checkpoints/refiner \
  --K 16 --hard_k 128 --sample_prefix_k 64 \
  --epochs 30 --eval_every 5 --rhs_only

说明：
- 评测时 eval_topk_inject 默认使用 score_delta_topk（R2 语义）。
- LHS 通常置 gamma_lhs=0（只对 RHS 开 Δ）。

Step 5. Train Gate（自适应注入）

目的：学习语义注入强度 g，避免固定 b 造成负优化。
输入：RotatE ckpt + Sem ckpt + cache（可选 Refiner 仅作特征）。

命令（示例）：
python train/train_gate_inject_topk.py \
  --data_path data/fb15k_custom \
  --pretrained_rotate artifacts/<run_id>/checkpoints/rotate/best_model.pth \
  --pretrained_sem artifacts/<run_id>/checkpoints/sem_biencoder/biencoder_best.pth \
  --train_cache_rhs artifacts/<run_id>/cache/train_rhs_top500_neg_filtered_trainvalid.pt \
  --train_cache_lhs artifacts/<run_id>/cache/train_lhs_top500_neg_filtered_trainvalid.pt \
  --save_dir artifacts/<run_id>/checkpoints/gate \
  --b_rhs 2.0 --b_lhs 2.5 \
  --epochs 5

产物：.../gate/gate_best.pth

Step 6. Final Eval（主表 + 消融 + bootstrap）

统一评测脚本：eval/eval_topk_inject.py
通用建议（论文数字统一开启）：
- --deterministic --disable_tf32 --matmul_precision highest --seed 42
- --batch_size 8 --score_eps 0
- 重要 run 一律 --save_ranks_path 便于比较与 bootstrap

必跑矩阵（system 口径）：
- R0 full-entity（RotatE）：eval_full_entity_filtered.py
- RotatE@TopK (strict)：eval_topk_inject --strict_r0（等价验证，不入主表）
- C：Sem+Gate（无 Δ）
- D：Sem+Gate+Δ
- C+SCN：Sem+Gate+r（可选但推荐）
- D+SCN：Sem+Gate+r+Δ（可选但推荐）
- Safety（可选）：--delta_gate_ent_q 0.6
- Sem-only strong@TopK（可选但推荐，用于堵审稿人）

可选消融（若需要补表）：
- Sem (TopK rerank; struct_weight=0)
- Δ-only（b=0, no gate）
- Hybrid LHS（--lhs_union_sem_topk）

Sem-only full-entity（单独表/附录）：
- 只用 eval_sem_biencoder_full.py，与 system 主表分开列出。

Paired bootstrap（显著性）：
- D − C（必须）
- C − RotatE@TopK、D − RotatE@TopK（建议）
- C − Sem-only strong@TopK（可选但很值）
命令关键参数：
--paired_bootstrap --paired_baseline_ranks <baseline>/ranks.pt

结果汇总：
- 主表与 system 对比块：artifacts/<run_id>/eval/paper_table.md
- Sem-only full-entity 独立放一张表或附录。

Δ‑X：LHS 增益解释与消融（需要提前写进论文）

机制解释（可直接写进 Method/Analysis）：
- LHS 基线更弱（结构排序更难、LHS Rec@K 更低），同样的纠错幅度带来更大的绝对增益。
- Δ‑X 使用 rank/score 序列 + 1‑hop 统计，不依赖 embedding；在 LHS（anchor=tail）上这些结构统计更具判别性，因此更容易修正。
- 过强 LHS 注入会破坏排序（gamma_lhs=2.0 明显退化），所以选用保守的 gamma_lhs=1.0；Rec@200 不变，排除“候选覆盖”导致的虚高。

Ablation A：w_lhs sweep（ComplEx，eval gamma_lhs=1.0）
| w_lhs | RHS MRR | LHS MRR | AVG MRR |
|---:|---:|---:|---:|
| 0.00* | 0.4849 | 0.3318 | 0.4083 |
| 0.25 | 0.5623 | 0.3750 | 0.4687 |
| 0.50 | 0.4703 | 0.3400 | 0.4051 |
| 1.00 | 0.4132 | 0.3376 | 0.3754 |
*w_lhs=0 为 RHS‑only 训练（不含 LHS loss）。

Ablation B：gamma_lhs sweep（固定 w_lhs=0.25）

ComplEx：
| gamma_lhs | RHS MRR | LHS MRR | AVG MRR |
|---:|---:|---:|---:|
| 0.0 | 0.5623 | 0.2085 | 0.3854 |
| 1.0 | 0.5623 | 0.3750 | 0.4687 |
| 2.0 | 0.5623 | 0.2447 | 0.4035 |

RotatE：
| gamma_lhs | RHS MRR | LHS MRR | AVG MRR |
|---:|---:|---:|---:|
| 0.0 | 0.5025 | 0.2216 | 0.3621 |
| 1.0 | 0.5025 | 0.4643 | 0.4834 |
| 2.0 | 0.5025 | 0.4384 | 0.4705 |

补充实验：TopK sweep（RotatE，test）

| K | Rec@K | R0 MRR | C MRR | D MRR | Δ‑X MRR | ΔMRR(Δ‑X−D) | ΔMRR(D−C) |
|---:|---:|---:|---:|---:|---:|---:|---:|
| 100 | 0.7699 | 0.3075 | 0.3202 | 0.3598 | 0.5762 | +0.2164 | +0.0395 |
| 200 | 0.8363 | 0.3075 | 0.3204 | 0.3598 | 0.4834 | +0.1236 | +0.0394 |
| 500 | 0.9096 | 0.3075 | 0.3204 | 0.3581 | 0.4377 | +0.0796 | +0.0377 |

结论：Rec@K 随 K 单调上升；Δ‑X 的增量在 K=100/200/500 均为正且随 K 增大趋于饱和。K=100 的高 MRR 提示模型对更短序列/更干净候选更敏感，因此补做 K‑敏感的 gamma_lhs sweep 解释该形态。

补充实验：RotatE 的 K‑敏感 gamma_lhs（test）

| K | gamma_lhs | AVG MRR | Rec@K |
|---:|---:|---:|---:|
| 100 | 0.0 | 0.4044 | 0.7700 |
| 100 | 1.0 | 0.5762 | 0.7700 |
| 500 | 0.0 | 0.3392 | 0.9096 |
| 500 | 1.0 | 0.4377 | 0.9096 |
| 500 | 2.0 | 0.4213 | 0.9096 |

观察：K=100 在 gamma_lhs=1.0 时显著高于 gamma_lhs=0；K=500 在 gamma_lhs=2.0 反而下降，提示更大 K 需要更保守的注入强度。

补充实验：TopK sweep（ComplEx，test）

| K | Rec@K | R0 MRR | C MRR | D MRR | Δ‑X MRR | ΔMRR(Δ‑X−D) | ΔMRR(D−C) |
|---:|---:|---:|---:|---:|---:|---:|---:|
| 100 | 0.7012 | 0.2739 | 0.3067 | 0.3680 | 0.4189 | +0.0509 | +0.0613 |
| 200 | 0.7695 | 0.2739 | 0.3052 | 0.3546 | 0.4687 | +0.1141 | +0.0494 |
| 500 | 0.8573 | 0.2739 | 0.3046 | 0.3211 | 0.4764 | +0.1552 | +0.0165 |

结论：Rec@K 单调上升；Δ‑X 对 D 的增益在 K=100/200/500 均为正，且在大 K 也不消失，支持 backbone‑agnostic 的稳健性。

补充实验：粗粒度效率（RotatE，K=200，test，batch_size=8）

| Setting | Wall‑clock (s) | 相对 R0 |
|---|---:|---:|
| R0 | 70.51 | 1.00× |
| C (Sem+Gate) | 137.16 | 1.95× |
| D (Sem+Gate+Δ‑R2) | 151.27 | 2.15× |
| Δ‑X (Sem+Gate+Δ‑X) | 150.82 | 2.14× |

备注：语义/Δ 仅作用于 Top‑K，整体仍远低于“全实体语义打分”的 O(|E|) 成本；Δ‑X 与 Δ‑R2 的额外开销同量级。

你们最容易被“瞎改”搞崩的三条红线（写给 Cursor）
1) Sem 训练时不能删掉 Sem-only full-entity 评测（A）。这是消融核心证据。
2) system rank 的阈值必须是 sem-preserving：s_thresh = struct_w*s_gold_struct + (b*g)*s_gold_sem（Δ 不进阈值）。
3) 任意改动 eval 逻辑后，先跑 strict 等价验收（check_eval_equivalence.sh）再汇报数字。

附录：ComplEx 结构基座（通用性补充）

目标：在同一 system protocol 下替换结构基座为 ComplEx，验证 C/D 与 SCN/Δ 增益仍成立。

Step A. Train ComplEx（结构基座）
python train/train_complex.py \
  --data_path data/fb15k_custom \
  --save_dir artifacts/<run_id>/checkpoints/complex \
  --emb_dim 500 \
  --batch_size 1024 --num_neg 256 \
  --lr 1e-4 --epochs 200 \
  --negative_adversarial_sampling --adversarial_temperature 1.0 \
  --eval_every 10 --eval_split valid

Step B. Cache（ComplEx topK negatives）
python build/build_struct_cache_rhs_topk.py \
  --data_path data/fb15k_custom \
  --struct_type complex \
  --pretrained_struct artifacts/<run_id>/checkpoints/complex/best_model.pth \
  --emb_dim 500 --K 500 --use_filtered --filtered_splits train_valid \
  --out_path artifacts/<run_id>/cache/train_rhs_top500_neg_filtered_trainvalid_complex.pt

python build/build_struct_cache_lhs_topk.py \
  --data_path data/fb15k_custom \
  --struct_type complex \
  --pretrained_struct artifacts/<run_id>/checkpoints/complex/best_model.pth \
  --emb_dim 500 --K 500 --use_filtered_trainvalid \
  --out_path artifacts/<run_id>/cache/train_lhs_top500_neg_filtered_trainvalid_complex.pt

Step C. Train Sem/Gate/SCN/Δ（全部切到 complex）
训练脚本保持不变，只需传：
- --struct_type complex
- --pretrained_struct artifacts/<run_id>/checkpoints/complex/best_model.pth
- cache 路径换成 *_complex.pt

Step D. Eval（system 口径）
python eval/eval_full_entity_filtered.py \
  --data_path data/fb15k_custom \
  --struct_type complex \
  --pretrained_struct artifacts/<run_id>/checkpoints/complex/best_model.pth \
  --eval_split test --recall_k 200 \
  --out_dir artifacts/<run_id>/eval/R0_complex_test \
  --save_ranks_path artifacts/<run_id>/eval/R0_complex_test/ranks.pt

python eval/eval_topk_inject.py \
  --data_path data/fb15k_custom \
  --struct_type complex \
  --pretrained_struct artifacts/<run_id>/checkpoints/complex/best_model.pth \
  --pretrained_sem <...> --pretrained_gate <...> --pretrained_scn <...> --pretrained_refiner <...> \
  --topk 200 --eval_split test --eval_sides both \
  --b_rhs 2.0 --b_lhs 2.5 \
  --refiner_gamma_rhs 2.0 --refiner_gamma_lhs 0.0 \
  --out_dir artifacts/<run_id>/eval/D_complex_sem_gate_delta_scn

附录：TopK 修正对拍（Full test, 口径一致性证明）

我们对 “TopK injection exact-rank” 的修正公式做了全 test 的 brute-force 对拍：显式构造全实体 total_all（结构全体分数 + TopK 位置替换为 total_topk），再做 filtered rank 计数；与脚本中的修正公式逐 query 完全一致。RHS/LHS 均无 mismatch，TopK 内无 filtered truth 漏入；RotatE 在 deterministic 设置下重复 2 次仍保持一致，排除偶然性/非确定性导致的虚高疑虑。

| Backbone | Side | mismatch_count | max_rank_diff | max_abs_score_diff | filt_in_topk_max |
|---|---|---:|---:|---:|---:|
| ComplEx (run_id=201) | RHS | 0 | 0 | 0.0 | 0 |
| ComplEx (run_id=201) | LHS | 0 | 0 | 0.0 | 0 |
| RotatE (run_id=200) | RHS | 0 | 0 | 0.0 | 0 |
| RotatE (run_id=200) | LHS | 0 | 0 | 0.0 | 0 |
